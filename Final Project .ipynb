{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "filename = \"/Users/patricktaylor/Documents/comp562/training_set_rel3.csv\"\n",
    "with open(filename, encoding=\"utf8\") as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    #get each column titles \n",
    "    headers = next(csvreader) \n",
    "    line_count = 0\n",
    "    rows = []\n",
    "    for row in csvreader:\n",
    "        #now have a list of lists to work with\n",
    "        rows.append(row) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract each set of essays  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only looking at these four \n",
    "set3 = []\n",
    "set4 = []\n",
    "set5 = []\n",
    "set6 = []\n",
    "for row in range(len(rows)):\n",
    "    #extract all data from set 3 essays \n",
    "    if rows[row][1] == '3':\n",
    "        if rows[row][6] != '': #ensure the essay was scored \n",
    "            set3.append(rows[row])\n",
    "    #extract all data from set 4 essays\n",
    "    elif rows[row][1] == '4':\n",
    "        if rows[row][6] != '':\n",
    "            set4.append(rows[row])\n",
    "    #extract all data from set 5 essays \n",
    "    elif rows[row][1] == '5':\n",
    "        if rows[row][6] != '':\n",
    "            set5.append(rows[row])\n",
    "    #extract all data from set 6 essays \n",
    "    elif rows[row][1] == '6':\n",
    "        if rows[row][6] != '':\n",
    "            set6.append(rows[row])\n",
    "#remove empty cells from each set \n",
    "for row in set3:\n",
    "    while '' in row :\n",
    "        row.remove('')\n",
    "for row in set4:\n",
    "    while '' in row :\n",
    "        row.remove('')\n",
    "for row in set5:\n",
    "    while '' in row :\n",
    "        row.remove('')\n",
    "for row in set6:\n",
    "    while '' in row :\n",
    "        row.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Reading Ease Feature (must be done before filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "#will store features in gl variables for now \n",
    "gl3 = []\n",
    "for i in range(len(set3)):\n",
    "    text = set3[i][2] \n",
    "    gf = textstat.flesch_reading_ease(text)\n",
    "    if gf > 100: #truncate scores\n",
    "        gf = 100\n",
    "    elif gf < 0:\n",
    "        gf = 0\n",
    "    gf = round(gf//5 * 5) #make it a multiple of 5 to reduce noise\n",
    "    gl3.append(gf)\n",
    "gl4 = []\n",
    "for i in range(len(set4)):\n",
    "    text = set4[i][2] \n",
    "    gf = textstat.flesch_reading_ease(text)\n",
    "    if gf > 100:\n",
    "        gf = 100\n",
    "    elif gf < 0:\n",
    "        gf = 0\n",
    "    gf = round(gf//5 * 5)\n",
    "    gl4.append(gf)\n",
    "gl5 = []\n",
    "for i in range(len(set5)):\n",
    "    text = set5[i][2] \n",
    "    gf = textstat.flesch_reading_ease(text)\n",
    "    if gf > 100:\n",
    "        gf = 100\n",
    "    elif gf < 0:\n",
    "        gf = 0\n",
    "    gf = round(gf//5 * 5)\n",
    "    gl5.append(gf)\n",
    "gl6 = []\n",
    "for i in range(len(set6)):\n",
    "    text = set6[i][2] \n",
    "    gf = textstat.flesch_reading_ease(text)\n",
    "    if gf > 100:\n",
    "        gf = 100\n",
    "    elif gf < 0:\n",
    "        gf = 0\n",
    "    gf = round(gf//5 * 5)\n",
    "    gl6.append(gf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out non alphabetical text in each essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out anything that isn't a-z, A-Z or space\n",
    "import re\n",
    "#for set 3 \n",
    "for i in range(len(set3)):\n",
    "    set3[i][2] = re.sub(r\"[^a-zA-Z ]+\", ' ', set3[i][2])\n",
    "#for set 4\n",
    "for i in range(len(set4)):\n",
    "    set4[i][2] = re.sub(r\"[^a-zA-Z ]+\", ' ', set4[i][2])\n",
    "#for set 5\n",
    "for i in range(len(set5)):\n",
    "    set5[i][2] = re.sub(r\"[^a-zA-Z ]+\", ' ', set5[i][2])\n",
    "#for set 6\n",
    "for i in range(len(set6)):\n",
    "    set6[i][2] = re.sub(r\"[^a-zA-Z ]+\", ' ', set6[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize sentence and Filter out stop words in each essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out stop words \n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english'))\n",
    "#for set3\n",
    "for i in range(len(set3)):\n",
    "    text = set3[i][2]\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    set3[i][2] = ' '.join(filtered_sentence)\n",
    "#for set4\n",
    "for i in range(len(set4)):\n",
    "    text = set4[i][2]\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    set4[i][2] = ' '.join(filtered_sentence)\n",
    "#for set5\n",
    "for i in range(len(set5)):\n",
    "    text = set5[i][2]\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    set5[i][2] = ' '.join(filtered_sentence)\n",
    "#for set6\n",
    "for i in range(len(set6)):\n",
    "    text = set6[i][2]\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    set6[i][2] = ' '.join(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Word Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set3 word count feature\n",
    "wordcount3 = []\n",
    "for i in range(len(set3)):\n",
    "    text = set3[i][2]\n",
    "    text = text.split(' ')\n",
    "    fdist = len(text)\n",
    "    wordcount3.append(fdist)\n",
    "#set4 word count feature \n",
    "wordcount4 = []\n",
    "for i in range(len(set4)):\n",
    "    text = set4[i][2]\n",
    "    text = text.split(' ')\n",
    "    fdist = len(text)\n",
    "    wordcount4.append(fdist)\n",
    "#set5 word count feature \n",
    "wordcount5 = []\n",
    "for i in range(len(set5)):\n",
    "    text = set5[i][2]\n",
    "    text = text.split(' ')\n",
    "    fdist = len(text)\n",
    "    wordcount5.append(fdist)\n",
    "wordcount6 = []\n",
    "for i in range(len(set6)):\n",
    "    text = set6[i][2]\n",
    "    text = text.split(' ')\n",
    "    fdist = len(text)\n",
    "    wordcount6.append(fdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Part of Speech Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/patricktaylor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/patricktaylor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#POS for set3\n",
    "pos3 = []\n",
    "for i in range(len(set3)):\n",
    "    text = set3[i][2]\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    text = nltk.Text(tokens)\n",
    "    tags = nltk.pos_tag(text)\n",
    "    counts = Counter(tag for word,tag in tags)\n",
    "    pos3.append(counts)\n",
    "#POS for set4\n",
    "pos4 = []\n",
    "for i in range(len(set4)):\n",
    "    text = set4[i][2]\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    text = nltk.Text(tokens)\n",
    "    tags = nltk.pos_tag(text)\n",
    "    counts = Counter(tag for word,tag in tags)\n",
    "    pos4.append(counts)\n",
    "#POS for set5\n",
    "pos5 = []\n",
    "for i in range(len(set5)):\n",
    "    text = set5[i][2]\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    text = nltk.Text(tokens)\n",
    "    tags = nltk.pos_tag(text)\n",
    "    counts = Counter(tag for word,tag in tags)\n",
    "    pos5.append(counts)\n",
    "#POS for set6\n",
    "pos6 = []\n",
    "for i in range(len(set6)):\n",
    "    text = set6[i][2]\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    text = nltk.Text(tokens)\n",
    "    tags = nltk.pos_tag(text)\n",
    "    counts = Counter(tag for word,tag in tags)\n",
    "    pos6.append(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up POS into individual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns3 = []\n",
    "verbs3 = []\n",
    "adj3 = []\n",
    "adv3 = []\n",
    "for i in range(len(set3)):\n",
    "    Pos = pos3[i]\n",
    "    nounCount = 0\n",
    "    verbCount = 0\n",
    "    adjCount = 0\n",
    "    advCount = 0\n",
    "    for p in Pos:\n",
    "        #nouns\n",
    "        if p[0] == 'N':\n",
    "            nounCount = nounCount + Pos[p]\n",
    "        #verbs\n",
    "        elif p[0] == 'V':\n",
    "            verbCount = verbCount + Pos[p]\n",
    "        #adjectives\n",
    "        elif p[0] == 'J':\n",
    "            adjCount = adjCount + Pos[p]\n",
    "        #adverb\n",
    "        elif p[0] == 'R':\n",
    "            advCount = advCount + Pos[p]\n",
    "    nouns3.append(nounCount)\n",
    "    verbs3.append(verbCount)\n",
    "    adj3.append(adjCount)\n",
    "    adv3.append(advCount)\n",
    "nouns4 = []\n",
    "verbs4 = []\n",
    "adj4 = []\n",
    "adv4 = []\n",
    "for i in range(len(set4)):\n",
    "    Pos = pos4[i]\n",
    "    nounCount = 0\n",
    "    verbCount = 0\n",
    "    adjCount = 0\n",
    "    advCount = 0\n",
    "    for p in Pos:\n",
    "        #nouns\n",
    "        if p[0] == 'N':\n",
    "            nounCount = nounCount + Pos[p]\n",
    "        #verbs\n",
    "        elif p[0] == 'V':\n",
    "            verbCount = verbCount + Pos[p]\n",
    "        #adjectives\n",
    "        elif p[0] == 'J':\n",
    "            adjCount = adjCount + Pos[p]\n",
    "        #adverb\n",
    "        elif p[0] == 'R':\n",
    "            advCount = advCount + Pos[p]\n",
    "    nouns4.append(nounCount)\n",
    "    verbs4.append(verbCount)\n",
    "    adj4.append(adjCount)\n",
    "    adv4.append(advCount)\n",
    "nouns5 = []\n",
    "verbs5 = []\n",
    "adj5 = []\n",
    "adv5 = []\n",
    "for i in range(len(set5)):\n",
    "    Pos = pos5[i]\n",
    "    nounCount = 0\n",
    "    verbCount = 0\n",
    "    adjCount = 0\n",
    "    advCount = 0\n",
    "    for p in Pos:\n",
    "        #nouns\n",
    "        if p[0] == 'N':\n",
    "            nounCount = nounCount + Pos[p]\n",
    "        #verbs\n",
    "        elif p[0] == 'V':\n",
    "            verbCount = verbCount + Pos[p]\n",
    "        #adjectives\n",
    "        elif p[0] == 'J':\n",
    "            adjCount = adjCount + Pos[p]\n",
    "        #adverb\n",
    "        elif p[0] == 'R':\n",
    "            advCount = advCount + Pos[p]\n",
    "    nouns5.append(nounCount)\n",
    "    verbs5.append(verbCount)\n",
    "    adj5.append(adjCount)\n",
    "    adv5.append(advCount)\n",
    "nouns6 = []\n",
    "verbs6 = []\n",
    "adj6 = []\n",
    "adv6 = []\n",
    "for i in range(len(set6)):\n",
    "    Pos = pos6[i]\n",
    "    nounCount = 0\n",
    "    verbCount = 0\n",
    "    adjCount = 0\n",
    "    advCount = 0\n",
    "    for p in Pos:\n",
    "        #nouns\n",
    "        if p[0] == 'N':\n",
    "            nounCount = nounCount + Pos[p]\n",
    "        #verbs\n",
    "        elif p[0] == 'V':\n",
    "            verbCount = verbCount + Pos[p]\n",
    "        #adjectives\n",
    "        elif p[0] == 'J':\n",
    "            adjCount = adjCount + Pos[p]\n",
    "        #adverb\n",
    "        elif p[0] == 'R':\n",
    "            advCount = advCount + Pos[p]\n",
    "    nouns6.append(nounCount)\n",
    "    verbs6.append(verbCount)\n",
    "    adj6.append(adjCount)\n",
    "    adv6.append(advCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create feature for how similar each essay is to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Jaccard distance between prompt and each essay \n",
    "import nltk\n",
    "jaccard3 = []\n",
    "prompt3 = \"Write a response that explains how the features of the setting affect the cyclist. In your response, include examples from the essay that support your conclusion.\"\n",
    "for i in range(len(set3)):\n",
    "    text = set3[i][2]\n",
    "    jaccard = nltk.jaccard_distance(set(prompt3), set(text))\n",
    "    jaccard = round(jaccard, 2)\n",
    "    jaccard3.append(jaccard)\n",
    "jaccard4 = []\n",
    "prompt4 = \"Write a response that explains why the author concludes the story with this paragraph. In your response, include details and examples from the story that support your ideas. \"\n",
    "for i in range(len(set4)):\n",
    "    text = set4[i][2]\n",
    "    jaccard = nltk.jaccard_distance(set(prompt4), set(text))\n",
    "    jaccard = round(jaccard, 2)\n",
    "    jaccard4.append(jaccard)\n",
    "jaccard5 = []\n",
    "prompt5 = \"Descibe the mood created by the author in the memoir. Support your answer with relevant and specific information from the memoir.\"\n",
    "for i in range(len(set5)):\n",
    "    text = set5[i][2]\n",
    "    jaccard = nltk.jaccard_distance(set(prompt5), set(text))\n",
    "    jaccard = round(jaccard, 2)\n",
    "    jaccard5.append(jaccard)\n",
    "jaccard6 = []\n",
    "prompt6 = \"Based on the excerpt, describe the obstacles the builders of the Empire State Building faced in attempting to allow dirigibles to dock there. Support your answer with relevant and specific information from the excerpt.\"\n",
    "for i in range(len(set6)):\n",
    "    text = set6[i][2]\n",
    "    jaccard = nltk.jaccard_distance(set(prompt6), set(text))\n",
    "    jaccard = round(jaccard, 2)\n",
    "    jaccard6.append(jaccard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add features to essay lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(set3)):\n",
    "    #add word count \n",
    "    set3[i].append(wordcount3[i])\n",
    "    #add readibility\n",
    "    set3[i].append(gl3[i])\n",
    "    #add nouns \n",
    "    set3[i].append(nouns3[i])\n",
    "    #add verbs \n",
    "    set3[i].append(verbs3[i])\n",
    "    #add adjectives\n",
    "    set3[i].append(adj3[i])\n",
    "    #add adverbs \n",
    "    set3[i].append(adv3[i])\n",
    "    #add jaccard\n",
    "    set3[i].append(jaccard3[i])\n",
    "for i in range(len(set4)):\n",
    "    #add word count \n",
    "    set4[i].append(wordcount4[i])\n",
    "    #add readibility\n",
    "    set4[i].append(gl4[i])\n",
    "    #add nouns \n",
    "    set4[i].append(nouns4[i])\n",
    "    #add verbs \n",
    "    set4[i].append(verbs4[i])\n",
    "    #add adjectives\n",
    "    set4[i].append(adj4[i])\n",
    "    #add adverbs \n",
    "    set4[i].append(adv4[i])\n",
    "    #add jaccard\n",
    "    set4[i].append(jaccard4[i])\n",
    "for i in range(len(set5)):\n",
    "    set5[i].append(wordcount5[i])\n",
    "    #add readibility\n",
    "    set5[i].append(gl5[i])\n",
    "    #add nouns \n",
    "    set5[i].append(nouns5[i])\n",
    "    #add verbs \n",
    "    set5[i].append(verbs5[i])\n",
    "    #add adjectives\n",
    "    set5[i].append(adj5[i])\n",
    "    #add adverbs \n",
    "    set5[i].append(adv5[i])\n",
    "    #add jaccard\n",
    "    set5[i].append(jaccard5[i])\n",
    "for i in range(len(set6)):\n",
    "    set6[i].append(wordcount6[i])\n",
    "    #add readibility\n",
    "    set6[i].append(gl6[i])\n",
    "    #add nouns \n",
    "    set6[i].append(nouns6[i])\n",
    "    #add verbs \n",
    "    set6[i].append(verbs6[i])\n",
    "    #add adjectives\n",
    "    set6[i].append(adj6[i])\n",
    "    #add adverbs \n",
    "    set6[i].append(adv6[i])\n",
    "    #add jaccard\n",
    "    set6[i].append(jaccard6[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create csv files for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to respective train and test csv files\n",
    "filename1 = 'EssaySet3Train.csv'\n",
    "headers = ['essays_id', 'essay_set', 'essay', 'rater1_domain1', 'rater2_domain1', 'domain1_score', 'word_count','reading_ease', 'nouns', 'verb', 'adjectives', 'adverbs', 'jaccard']\n",
    "with open(filename1, 'w', encoding=\"utf-8\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    #write headers \n",
    "    csvwriter.writerow(headers) \n",
    "    #writerows\n",
    "    for i in range(len(set3)-518):\n",
    "        csvwriter.writerow(set3[i])\n",
    "filename1 = 'EssaySet3Test.csv'\n",
    "with open(filename1, 'w', encoding=\"utf-8\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    #write headers \n",
    "    csvwriter.writerow(headers) \n",
    "    #writerows\n",
    "    for i in range(1208, len(set3)):\n",
    "        csvwriter.writerow(set3[i])\n",
    "filename1 = 'EssaySet4Train.csv'\n",
    "with open(filename1, 'w', encoding=\"utf-8\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    #write headers \n",
    "    csvwriter.writerow(headers) \n",
    "    #writerows\n",
    "    for i in range(len(set4)-532):\n",
    "        csvwriter.writerow(set4[i])\n",
    "filename1 = 'EssaySet4Test.csv'\n",
    "with open(filename1, 'w', encoding=\"utf-8\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    #write headers \n",
    "    csvwriter.writerow(headers) \n",
    "    #writerows\n",
    "    for i in range(1240, len(set4)):\n",
    "        csvwriter.writerow(set4[i])\n",
    "filename1 = 'EssaySet5Train.csv'\n",
    "with open(filename1, 'w', encoding=\"utf-8\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    #write headers \n",
    "    csvwriter.writerow(headers) \n",
    "    #writerows\n",
    "    for i in range(len(set5)-542):\n",
    "        csvwriter.writerow(set5[i])\n",
    "filename1 = 'EssaySet5Test.csv'\n",
    "with open(filename1, 'w', encoding=\"utf-8\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    #write headers \n",
    "    csvwriter.writerow(headers) \n",
    "    #writerows\n",
    "    for i in range(1263, len(set5)):\n",
    "        csvwriter.writerow(set5[i])\n",
    "filename1 = 'EssaySet6Train.csv'\n",
    "with open(filename1, 'w', encoding=\"utf-8\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    #write headers \n",
    "    csvwriter.writerow(headers) \n",
    "    #writerows\n",
    "    for i in range(len(set6)-680):\n",
    "        csvwriter.writerow(set6[i])\n",
    "filename1 = 'EssaySet6Test.csv'\n",
    "with open(filename1, 'w', encoding=\"utf-8\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, lineterminator = '\\n') \n",
    "    #write headers \n",
    "    csvwriter.writerow(headers) \n",
    "    #writerows\n",
    "    for i in range(1120, len(set6)):\n",
    "        csvwriter.writerow(set6[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essay set 3 accuracy: 0.666023166023166\n",
      "Essay set 4 accuracy: 0.5706214689265536\n",
      "Essay set 5 accuracy: 0.6881918819188192\n",
      "Essay set 6 accuracy: 0.5823529411764706\n",
      "mean accuracy for SVM: 0.6267973645112523\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "#set3 \n",
    "score = pd.read_csv(\"EssaySet3Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet3Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "model = svm.SVC()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "pred = model.predict(features_test)\n",
    "accuracy3 = accuracy_score(target_test.values.ravel(), pred)\n",
    "print(\"Essay set 3 accuracy:\", accuracy3)\n",
    "#set4\n",
    "score = pd.read_csv(\"EssaySet4Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet4Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "model = svm.SVC()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "pred = model.predict(features_test)\n",
    "accuracy4 = accuracy_score(target_test.values.ravel(), pred)\n",
    "print(\"Essay set 4 accuracy:\", accuracy4)\n",
    "#set5\n",
    "score = pd.read_csv(\"EssaySet5Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet5Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "model = svm.SVC()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "pred = model.predict(features_test)\n",
    "accuracy5 = accuracy_score(target_test.values.ravel(), pred)\n",
    "print(\"Essay set 5 accuracy:\", accuracy5)\n",
    "#set5\n",
    "score = pd.read_csv(\"EssaySet6Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet6Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "model = svm.SVC()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "pred = model.predict(features_test)\n",
    "accuracy6 = accuracy_score(target_test.values.ravel(), pred)\n",
    "print(\"SVM Essay set 6 accuracy:\", accuracy6)\n",
    "print(f'mean accuracy for SVM: {np.mean([accuracy3,accuracy4,accuracy5,accuracy6])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essay set 3 accuracy: 0.667953667953668\n",
      "Essay set 4 accuracy: 0.5932203389830508\n",
      "Essay set 5 accuracy: 0.6291512915129152\n",
      "Essay set 6 accuracy: 0.5588235294117647\n",
      "mean accuracy for linear regression: 0.6122872069653497\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#set3 \n",
    "score = pd.read_csv(\"EssaySet3Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet3Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "model = GaussianNB()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "pred = model.predict(features_test)\n",
    "accuracy3 = accuracy_score(target_test.values.ravel(), pred)\n",
    "print(\"Essay set 3 accuracy:\", accuracy3)\n",
    "#set4\n",
    "score = pd.read_csv(\"EssaySet4Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet4Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "model = GaussianNB()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "pred = model.predict(features_test)\n",
    "accuracy4 = accuracy_score(target_test.values.ravel(), pred)\n",
    "print(\"Essay set 4 accuracy:\", accuracy4)\n",
    "#set5\n",
    "score = pd.read_csv(\"EssaySet5Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet5Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "model = GaussianNB()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "pred = model.predict(features_test)\n",
    "accuracy5 = accuracy_score(target_test.values.ravel(), pred)\n",
    "print(\"Essay set 5 accuracy:\", accuracy5)\n",
    "#set5\n",
    "score = pd.read_csv(\"EssaySet6Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet6Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "model = GaussianNB()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "model.fit(features_train, target_train.values.ravel())\n",
    "pred = model.predict(features_test)\n",
    "accuracy6 = accuracy_score(target_test.values.ravel(), pred)\n",
    "print(\"naive bayes Essay set 6 accuracy:\", accuracy6)\n",
    "print(f'mean accuracy for bayes: {np.mean([accuracy3,accuracy4,accuracy5,accuracy6])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinReg Essay set 3 accuracy: 0.6505791505791506\n",
      "LinReg Essay set 4 accuracy: 0.5725047080979284\n",
      "LinReg Essay set 5 accuracy: 0.6660516605166051\n",
      "LinReg Essay set 6 accuracy: 0.5573529411764706\n",
      "mean accuracy for linear regression: 0.6116221150925387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "#set3 \n",
    "score = pd.read_csv(\"EssaySet3Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet3Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "#model = GaussianNB()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "reg= LinearRegression().fit(features_train, target_train.values.ravel())\n",
    "pred = reg.predict(features_test)\n",
    "#print(np.shape(pred))\n",
    "accuracy3 = accuracy_score(target_test.values.ravel(), np.round(pred).astype('int64'))\n",
    "print(\"LinReg Essay set 3 accuracy:\", accuracy3)\n",
    "#set4\n",
    "score = pd.read_csv(\"EssaySet4Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet4Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "#model = GaussianNB()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "reg=LinearRegression().fit(features_train, target_train.values.ravel())\n",
    "pred = reg.predict(features_test)\n",
    "accuracy4 = accuracy_score(target_test.values.ravel(), np.round(pred).astype('int64'))\n",
    "print(\"LinReg Essay set 4 accuracy:\", accuracy4)\n",
    "#set5\n",
    "score = pd.read_csv(\"EssaySet5Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet5Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "model = GaussianNB()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "reg=LinearRegression().fit(features_train, target_train.values.ravel())\n",
    "pred = reg.predict(features_test)\n",
    "accuracy5 = accuracy_score(target_test.values.ravel(), np.round(pred).astype('int64'))\n",
    "print(\"LinReg Essay set 5 accuracy:\", accuracy5)\n",
    "#set5\n",
    "score = pd.read_csv(\"EssaySet6Train.csv\")\n",
    "test = pd.read_csv(\"EssaySet6Test.csv\")\n",
    "features = [\"word_count\", \"reading_ease\", \"nouns\", \"verb\", \"adjectives\", \"adverbs\", \"jaccard\"]\n",
    "target = [\"domain1_score\"]\n",
    "#model = GaussianNB()\n",
    "features_train = score[features]\n",
    "target_train = score[target]\n",
    "features_test = test[features]\n",
    "target_test = test[target]\n",
    "reg=LinearRegression().fit(features_train, target_train.values.ravel())\n",
    "pred = reg.predict(features_test)\n",
    "accuracy6 = accuracy_score(target_test.values.ravel(), np.round(pred).astype('int64'))\n",
    "print(\"LinReg Essay set 6 accuracy:\", accuracy6)\n",
    "\n",
    "print(f'mean accuracy for linear regression: {np.mean([accuracy3,accuracy4,accuracy5,accuracy6])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
